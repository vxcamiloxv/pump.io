#!/usr/bin/env node

// -*- mode: javascript -*-

// pump.io
//
// entry point activity pump application
//
// Copyright 2011-2012, E14N https://e14n.com/
// Copyright 2017-2018 AJ Jordan <alex@strugee.net>
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

"use strict";

var cluster = require("cluster"),
    os = require("os"),
    fs = require("fs"),
    path = require("path"),
    net = require("net"),
    Logger = require("bunyan"),
    _ = require("lodash"),
    split = require("split"),
    onExit = require("signal-exit"),
    pkg = require("../package"),
    configUtil = require("../lib/config"),
    restartUtil = require("../lib/restartutil"),
    bunyanLevels = ["fatal", "error", "warn", "info", "debug", "trace"],
    restartInflight = false,
    abortedRestart = false,
    restartedWorker = null,
    argv = require("yargs")
           .usage("Usage: $0 -c <configfile>")
           .alias("c", "config")
           // Hack to make sure that .config()'s callback gets run even if -c isn't specified
           .default("c", "/dev/null", "/etc/pump.io.json and ~/.pump.io.json")
           .config("c", "Configuration file", function findConfig(filename) {
               var files,
                   config = {},
                   i,
                   raw,
                   parsed;

               if (filename !== "/dev/null") {
                   files = [filename];
               } else {
                   files = ["/etc/pump.io.json"];
                   if (process.env.HOME) {
                       files.push(path.join(process.env.HOME, ".pump.io.json"));
                   }
               }

               // This is all sync
               for (i = 0; i < files.length; i++) {
                   try {
                       raw = fs.readFileSync(files[i]);
                   } catch (err) {
                       continue;
                   }

                   try {
                       parsed = JSON.parse(raw);
                       _.extend(config, parsed);
                   } catch (err) {
                       console.error("Error parsing JSON configuration:", err.toString());
                       console.error("Try using a JSON validator.");
                       process.exit(1);
                   }
               }

               return config;
           })
           .env("PUMPIO")
           // TODO more validation of booleans, etc. here
           .number(["port", "urlPort", "smtpport", "smtptimeout", "cleanupSession", "cleanupNonce"])
           .string(["driver", "hostname", "address", "secret", "site", "owner", "ownerURL", "mainImage", "appendFooter", "logfile", "logLevel", "serverUser", "key", "cert", "datadir", "firehose", "spamhost", "spamclientid", "spamclientsecret", "smtpserver", "smtpuser", "smtppass", "smtpfrom", "favicon", "controlSocket"])
           .boolean(["bounce", "noweb", "nologger", "enableUploads", "debugClient", "disableRegistration", "noCDN", "requireEmail", "smtpusetls", "smtpusessl", "compress", "sockjs"])
           .alias("help", "h")
           .help()
           .alias("version", "v")
           .version()
           .argv,
    Dispatch = require("../lib/dispatch");

// We first get config files and launch some cluster apps

var main = function() {
    var config = argv;
    launchApps(config);
};

// Launch apps

var launchApps = function(configBase) {
    var config = configUtil.buildConfig(configBase),
        i,
        log = setupLog(config),
        clusterLog = log.child({component: "cluster"});

    if (cluster.isMaster) {

        Dispatch.start(log);

        for (i = 0; i < config.workers; i++) {
            cluster.fork();
        }

        // Restart child processes when they die

        cluster.on("exit", function(worker, code, signal) {
            var zeroDowntimeRestart = worker.exitedAfterDisconnect,
                exitCode = worker.process.exitCode;

            // Don't respawn any workers if we aborted a restart
            // Note that even for crashes it doesn't make sense to respawn since we _know_ all new workers will be bad
            if (abortedRestart) {
                clusterLog.error("worker " + worker.process.pid + " died ("+exitCode+") but refusing to restart due to aborted zero-downtime restart; please restart this process");
                return;
            }

            // Check if a zero-downtime worker respawn went bad
            if (worker.process.pid === restartedWorker) {
                clusterLog.error("worker " + worker.process.pid + " died ("+exitCode+") directly after respawn for zero-downtime restart");
                abortedRestart = true;
                restartedWorker = null;
            }

            if (!zeroDowntimeRestart) {
                clusterLog.error("worker " + worker.process.pid + " died ("+exitCode+"). restarting...");
            }

            cluster.fork();
        });

        var ctl = net.createServer(function(con) {
            con.on("error", function(err) {
                /*
                  Suppress errors when we write() after the
                  connection is closed. This is just easier than
                  guarding all write() calls on con.writable.
                */
                if (err.code !== "EPIPE") throw err;
            });

            /*
              Note that we are totally abusing Writable#write() by not
              checking the return value (and potentially waiting for
              the "drain" event), but it's okay because we're only
              writing a small amount of data and it makes stuff
              easier, and also since we're the original source we'd
              just buffer it in memory anyway and we'd have the exact
              same memory problem that backpressure is supposed to
              solve so who cares.
            */
            con.pipe(split()).on("data", function(line) {
                line = line.split(" ");
                switch (line[0].toLowerCase()) {
                case "cmds":
                    clusterLog.info("Received CMDS command over control socket.");
                    con.write("ACK CMDS VERSION EASTER LOGLEVEL RESTART\n");
                    break;
                case "version":
                    clusterLog.info("Received VERSION command over control socket.");
                    con.write("ACK " + pkg.version + "\n");
                    break;
                case "easter":
                    con.write("ACK EGG\n");
                    break;
                case "loglevel":
                    var level = line[1];

                    clusterLog.info("Received LOGLEVEL command over control socket.", {level: level});

                    if (level !== "restore" && !bunyanLevels.includes(level)) {
                        con.write("NACK invalid\n");
                        break;
                    }

                    if (level === "restore") {
                        level = config.logLevel;
                    }

                    clusterLog.level(level);
                    for (var id in cluster.workers) {
                        cluster.workers[id].send({cmd: "logLevel", level: level});
                    }

                    con.write("ACK\n");
                    break;
                case "restart":
                    clusterLog.info("Received RESTART command over control socket.");
                    var restartCheck = restartUtil.checkRequirements(config, abortedRestart, restartInflight);
                    if (!restartCheck.ok) {
                        con.write("NACK ");
                        switch (restartCheck.reason) {
                        case "no workers":
                            log.warn("Received restart request but ignoring because there aren't enough workers for zero-downtime restart");
                            con.write("no workers");
                            break;
                        case "bad driver":
                            log.warn("Zero-downtime restarts are only supported on MongoDB for the time being");
                            con.write("bad driver");
                            break;
                        case "aborted restart":
                            log.warn("Received restart request but refusing due to an earlier aborted restart; please restart this process");
                            con.write("aborted restart");
                            break;
                        case "restart inflight":
                            log.warn("Received restart request but ignoring because there's already an in-flight zero-downtime restart");
                            con.write("restart in flight");
                            break;
                        case "bad epoch":
                            log.warn("Refusing zero-downtime restart because the new version says it's incompatible with us");
                            con.write("incompatibility");
                            break;
                        default:
                            throw new Error("Unknown failure reason " + restartCheck.reason);
                        }
                        con.write("\n");
                        return;
                    }

                    triggerRestart(clusterLog, config);
                    con.write("ACK\n");
                    break;
                case "":
                    // Ignore this because clients apparently send it when closing the socket connection
                    break;
                default:
                    clusterLog.info("Received unknown command over control socket; rejecting.", {socketCmd: line});
                    con.write("NACK unknown\n");
                }
            });
        });

        ctl.listen(config.controlSocket, function() {
            clusterLog.info("Listening for control commands on path " + config.controlSocket + ".");
        });

        onExit(function() {
            try {
                fs.unlinkSync(config.controlSocket);
            } catch (e) {
                if (e.code !== "ENOENT") throw e;
            }
        });

    } else {

        // We load this down here instead of at the top so we don't implicitly execute all this code as root
        var makeApp = require("../lib/app").makeApp;

        makeApp(configBase, function(err, appServer) {
            if (err) {
                console.error(err);
                process.exit(1);
            } else {
                appServer.run(function(err) {
                    if (err) {
                        console.error(err);
                        process.exit(1);
                    }
                });
            }
        });
    }
};

// Initialize a zero-downtime restart

var triggerRestart = function(clusterLog, config) {
    restartInflight = true;

    clusterLog.info("Starting zero-downtime restart...");

    clusterLog.debug("Determining zero-downtime restart eligibility...");

    var reqs = restartUtil.checkRequirements(config, abortedRestart, restartInflight);
    if (!reqs.ok) return;

    var workers = [];
    for (var id in cluster.workers) {
        workers.push(cluster.workers[id]);
    }

    restartWorker(workers.pop(), workers, clusterLog, function() {
        clusterLog.info("Zero-downtime restart complete");
        restartInflight = false;
    });
};

// Restart a worker and schedule the next restart

var restartWorker = function(worker, workerQueue, log, cb) {
    worker.send({cmd: "gracefulShutdown"});
    log.debug("Killed " + worker.process.pid + " for zero-downtime restart.");

    var timeout = setTimeout(function() {
        log.warn("Killed " + worker.process.pid + " because it didn't shut down within 30 seconds.");
        worker.kill();
    }, 30 * 1000);

    worker.once("exit", function() {
        clearTimeout(timeout);
    });

    // TODO error handling?
    cluster.once("fork", function(worker) {
        log.debug("Respawned " + worker.process.pid + " with new code.");
        restartedWorker = worker.process.pid;

        // We don't use the 'listening' event and wait for
        // explicit notification because 'listen' would fire when
        // the main server was listening but not the bounce server
        worker.once("message", function(obj) {
            switch (obj.msg) {
            case "listening":
                if (workerQueue.length === 0) {
                    cb();
                    return;
                }

                restartWorker(workerQueue.pop(), workerQueue, log, cb);

                restartedWorker = null;
                break;
            case "abortRestart":
                log.warn(obj.err, "New worker signaled error; the master is in an inconsistent state and should be restarted ASAP");
                abortedRestart = true;
                break;
            default:
                // msg is an internal Bunyan field so we rename it
                obj.workerMsg = obj.msg;
                delete obj.msg;
                log.error(obj, "Received unknown restart status message from worker");
                throw new Error("Received unknown restart status message from worker: " + obj.workerMsg);
            }
        });
    });
};

// Set up a Bunyan logger that the dispatch code can send to

var setupLog = function(config) {

    var log,
        logParams = {
            name: "pump.io"
        };

    if (config.logfile) {
        logParams.streams = [{path: config.logfile}];
    } else if (config.nologger) {
        logParams.streams = [{path: "/dev/null"}];
    } else {
        logParams.streams = [{stream: process.stderr}];
    }

    logParams.streams[0].level = config.logLevel;

    return new Logger(logParams);
};

main();
